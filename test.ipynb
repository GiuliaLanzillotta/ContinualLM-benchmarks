{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.transformers.bayesian import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing on the embedding implemntation\n",
    "## Tokenlize model input: from batched sentences to batched sequence of code\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "# layer config \n",
    "d_model = 768\n",
    "d_embed = 1024  # Larger embedding dimension\n",
    "vocab_size=30522\n",
    "\n",
    "# loading sample data\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True, use_multiprocessing=False)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "max_position_embeddings = 512\n",
    "model_inputs = tokenizer(sequences, truncation=True,  padding=\"longest\")\n",
    "\n",
    "# Check vocabulary size from the tokenizer\n",
    "# Happen to be the same as the default setting for distilbert -- of course!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
    "\n",
    "\n",
    "input = torch.tensor(model_inputs['input_ids'])\n",
    "embedder = EmbeddingWithProjection(vocab_size=vocab_size, d_embed=d_embed, d_model=d_model)\n",
    "output = embedder(input)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Embedded shape after projection: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
